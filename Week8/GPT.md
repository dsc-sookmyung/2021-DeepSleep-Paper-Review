# Improving Language Understanding by Generative Pre-Training

### Generative Pre-training of language model, GPT

**`Language Model`**

ì–¸ì–´ ëª¨ë¸(Language Model, LM)ì€ **ë‹¨ì–´ ì‹œí€€ìŠ¤ì— í™•ë¥ ì„ í• ë‹¹**í•˜ëŠ” ì¼ì„ í•˜ëŠ” ëª¨ë¸ë¡œ, **ì´ì „ ë‹¨ì–´ë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡**í•˜ë„ë¡ í•œë‹¤.

<img src="https://thegradient.pub/content/images/2019/10/lm-1.png" alt="img" width="70%" />

_ì¶œì²˜ https://thegradient.pub/understanding-evaluation-metrics-for-language-models/_

**`Generative Model vs Discriminative Model`**

<img src="https://datawarrior.files.wordpress.com/2016/05/discriminative_vs_generative.png" alt="img" width="40%" />

_ì¶œì²˜ https://datawarrior.wordpress.com/2016/05/08/generative-discriminative-pairs/_

- Discriminative model: ë ˆì´ë¸” ì •ë³´ê°€ ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì§€ë„í•™ìŠµ(supervised learning) ë²”ì£¼ì— ì†í•˜ë©°, ì£¼ì–´ì§„ ë°ì´í„°ì˜ ë ˆì´ë¸”ì„ ì˜ êµ¬ë¶„í•˜ëŠ” ê²°ì • ê²½ê³„(decision boundary)ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.
- Generative model: ë ˆì´ë¸” ì •ë³´ê°€ ìˆì–´ë„ ë˜ê³ , ì—†ì–´ë„ ë˜ë©° ë²”ì£¼ì˜ ë¶„í¬(distribution)ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.

_generative ëª¨ë¸ê³¼ discriminative ëª¨ë¸ì˜ ì°¨ì´ì ì€ [ì—¬ê¸°](https://ratsgo.github.io/generative%20model/2017/12/17/compare/)ì—ì„œ ìì„¸íˆ ì•Œì•„ë³¼ ìˆ˜ ìˆë‹¤._

## ë°°ê²½

ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ í•™ìŠµì€ ìˆ˜ë™ìœ¼ë¡œ ë¼ë²¨ë§ëœ ë°ì´í„°ê°€ í•„ìš”í•œë°, `labeled data`ê°€ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì—, discriminatively trained modelsê°€ ì˜ ì‘ë™í•˜ê¸° ì–´ë µë‹¤. 

ë”°ë¼ì„œ `unlabeled data`ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì´ ì‹œê°„ ì†Œëª¨ë‚˜ ë¹„ìš©ì„ ì¤„ì´ëŠ”ë° ì¢‹ì€ ëŒ€ì•ˆì´ ë˜ì—ˆë‹¤. 

ì§€ë‚œ ëª‡ ë…„ë™ì•ˆ, ì—°êµ¬ìë“¤ì€ `pre-trained word embedding`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ê°•ë ¥í•˜ë‹¤ê³  ì¦ëª…í•´ì™”ëŠ”ë°, ì´ëŠ” ì£¼ë¡œ ë‹¨ì–´ ìˆ˜ì¤€ì˜ ì •ë³´ë¥¼ transferí–ˆë‹¤.

í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ê³ ìˆ˜ì¤€ì˜ ì˜ë¯¸(higher-level semantics)ë¥¼ í¬ì°©í•´ì•¼í–ˆëŠ”ë°, ë¼ë²¨ë§ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ìˆ˜ì¤€ì„ ë„˜ì–´ì„œëŠ” ì •ë³´ë¥¼ ë½‘ì•„ë‚´ëŠ” ê²ƒì€ í¬ê²Œ ë‘ ê°€ì§€ ì´ìœ  ë•Œë¬¸ì— ì–´ë µë‹¤.

1.  ì–´ë–¤ ìµœì í™”ë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜(optimization objectives)ê°€ ì „ì´ í•™ìŠµì— ìœ ìš©í•œ text representationì„ í•™ìŠµí•˜ê¸°ì— ê°€ì¥ íš¨ê³¼ì ì¸ì§€ ë¶ˆëª…í™•í•˜ë‹¤.
2. í•™ìŠµëœ representationì„ target taskì— ì „ì´í•˜ëŠ” ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ì— ëŒ€í•œ í•©ì˜ê°€ ì—†ë‹¤.

ì´ ë¶ˆí™•ì‹¤ì„±ì€ ì–¸ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ íš¨ê³¼ì ì¸ semi-supervised í•™ìŠµ ë°©ë²•ì„ ë°œì „ì‹œí‚¤ëŠ” ë° ì–´ë ¤ì›€ì„ ì£¼ì—ˆë‹¤.

â‡’ ì´ ë…¼ë¬¸ì—ì„œëŠ” unsupervised pre-trainingê³¼ supervised fine-tuningì˜ ì¡°í•©ì„ ì´ìš©í•œ semi-supervised approachë¥¼ ì—°êµ¬í–ˆë‹¤.

**`ëª©í‘œ`**   `ì•½ê°„ì˜ fine-tuningë§Œìœ¼ë¡œ ë‹¤ì–‘í•œ taskì— ì˜ ì „ì´í•˜ëŠ”, ë²”ìš©ì ì¸(universal) representations í•™ìŠµí•˜ê¸°`

## Framework

### 2ë‹¨ê³„ í•™ìŠµ ê³¼ì • (Two-stages training procedure)

**1ë‹¨ê³„**

- Generative pre-training language model

- Unlabeled large corpusë¡œ í•™ìŠµ

**2ë‹¨ê³„**  

- Discriminative Fine-tuning Language Model
- Labeled data í™œìš©í•˜ì—¬ specific taskì— í•™ìŠµ

### Model architecture

- ê¸°ì¡´ Transformerì˜ decoderë¥¼ 12ê°œ ìŒ“ì€ êµ¬ì¡° (decoderì—ì„œ Multi-Head Attention ì œì™¸)

  â†’ TransformerëŠ” í…ìŠ¤íŠ¸ì˜ long-term dependenciesë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë³´ë‹¤ êµ¬ì¡°í™”ëœ ë©”ëª¨ë¦¬ë¥¼ ì œê³µí•œë‹¤.

  â†’ Masked Multi-Head Attentionì—ì„œ `Masking`ì€ ì–¸ì–´ ëª¨ë¸ì´ í˜„ì¬ ë‹¨ì–´ì˜ ì˜¤ë¥¸ìª½ì— ìˆëŠ” í›„ì† ë‹¨ì–´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ê²Œ í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤.

  <img src="https://i.imgur.com/Q7IS78n.png" alt="img" width="40%" />
  
  _ì¶œì²˜ https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/_

- transferë¥¼ í•˜ëŠ” ë™ì•ˆ, êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì…ë ¥ì„ ë‹¨ì¼ ì—°ì† í† í° ì‹œí€€ìŠ¤ë¡œ ì²˜ë¦¬í•˜ëŠ” traversal-style ì ‘ê·¼ ë°©ì‹ì—ì„œ íŒŒìƒëœ `task-specific input transformation`ì„ í™œìš©í•œë‹¤.

  â†’ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ ìµœì†Œí•œìœ¼ë¡œ ë³€ê²½í•˜ì—¬ íš¨ê³¼ì ìœ¼ë¡œ fine-tuneì„ í•  ìˆ˜ ìˆë‹¤.

### Step 1. Unsupervised pre-training

- Pre-training of  `Language Model`

  Unsupervised Learning with unlabeled text

  - use a standard language modeling objective (ê¸°ì¡´ì˜ LMê³¼ ê³µì‹ ê°™ìŒ)

    ![img](https://miro.medium.com/max/1168/1*Zrg8WFl_Zc7FDtSVgLC9eg.png)
  
    - Unsupervised corpus of token ğ‘‡ì— ëŒ€í•´ ğ‘˜(window size)ê°œ í† í°ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒ í† í° ì˜ˆì¸¡
    - ë‹¤ìŒ í† í°ì´ ë“±ì¥í•  likelihood L1(ğ‘‡)ë¥¼ ìµœëŒ€í™” í•˜ë„ë¡ í•™ìŠµ
  
  - use a multi-layer _Transformer decoder_ for the language model

### Step 2. Supervised fine-tuning

- Fine tuning on each specific task

  Supervised Learning

  - ì£¼ì–´ì§„ í† í°(x1, ..., xn)ì„ ì´ìš©í•˜ì—¬ label yë¥¼ ì˜ˆì¸¡í•  ê°€ëŠ¥ì„±ì„ ìµœëŒ€í™”

    ![img](https://miro.medium.com/max/1044/1*5hDwpxGf2KGPlNOvmcBX6g.png)

  - equation (ii)ë¥¼ ë°”ë¡œ ìµœëŒ€í™”í•˜ì§€ ì•Šê³ , ë³´ì¡° ëª©ì  í•¨ìˆ˜(auxiliary objective) ì‚¬ìš©

    ![img](https://miro.medium.com/max/1088/1*pFWB54O7V8HtWu97H0wUIw.png)

    - supervised modelì˜ ì¼ë°˜í™”(generalization) ë” ì˜ ë˜ê²Œ í•¨
    - ìˆ˜ë ´(convergence) ê°€ì†í™”

  - ê¸°ì¡´ ëª¨ë¸ë“¤ì€ fine-tuning ì‹œì— layerë¥¼ ì¶”ê°€í•´ì•¼í–ˆê³ , ì ì§€ ì•Šì€ ì‹œê°„ê³¼ ë¹„ìš©ì´ ì†Œëª¨ëœë‹¤.

    But, GPT-1ì€ **layer ì¶”ê°€ ì‘ì—… ì—†ì´** language model í•™ìŠµ ì‹œì— ì‚¬ìš©í•œ Transformer decoder ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ fine-tuningì—ë„ ìœ ì§€í•œë‹¤.

  - Fine-tuning ê³¼ì • ë™ì•ˆ ë ˆì´ë¸”ë§ì„ í†µí•´ ëª¨ë¸ì´ íŠ¹ì • taskì— ìµœì í™”ë˜ê²Œ í•œë‹¤.

![GPT1á„€á…®á„Œá…©](https://user-images.githubusercontent.com/53266682/130374807-1c4457f4-61d6-45f7-bb75-f423109f4bac.png)

### Task-specific input transformation

Classification ê°™ì€ ì¼ë¶€ taskëŠ” ë°”ë¡œ fine-tuneì´ ê°€ëŠ¥í•˜ë‹¤.

ë‹¤ë¥¸ íŠ¹ì • taskëŠ” êµ¬ì¡°í™”ëœ ì…ë ¥ì„ ê°–ê³  ìˆë‹¤. ë…¼ë¬¸ì˜ pre-trained ëª¨ë¸ì´ í…ìŠ¤íŠ¸ì˜ ì—°ì† ì‹œí€€ìŠ¤ë¡œ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì—, ì´ëŸ¬í•œ taskë“¤ì„ ì´ ëª¨ë¸ì— ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ìˆ˜ì •ì´ í•„ìš”í•˜ë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë…¼ë¬¸ì˜ pre-trained ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ, êµ¬ì¡°í™”ëœ ì…ë ¥ì„ ì •ë ¬ëœ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•´ì£¼ëŠ” traversal-style approachë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ ì…ë ¥ ë³€í™˜(input transformation)ì„ í†µí•´ ì‘ì—… ì „ë°˜ì— ê±¸ì³ ì•„í‚¤í…ì²˜ë¥¼ ìµœì†Œí•œìœ¼ë¡œ ë³€ê²½í•˜ì—¬ íš¨ê³¼ì ìœ¼ë¡œ fine-tuneì„ í•  ìˆ˜ ìˆë‹¤.

- ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ startì™€ end tokensë¥¼ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•œë‹¤.

- ë‘ ê°œì˜ ë¬¸ì¥ ì‚¬ì´ì— special character(delimeter)ë¥¼ ì§‘ì–´ë„£ê³  í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë¬¶ì–´ì„œ ëª¨ë¸ì˜ ì…ë ¥ layerì— ë„£ëŠ”ë‹¤.


## Experiments

### 1. Unsupervised Training

**`Dataset`**

- BooksCorpus datasetì„ ì´ìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ í•™ìŠµ
  - ë³¸ ì  ì—†ëŠ” ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ë° ë„ì›€ì´ ëœ ì•½ 7000ê¶Œì˜ ë¯¸ì¶œê°„ ì±…ìœ¼ë¡œ êµ¬ì„±
  - ë§ë­‰ì¹˜(corpus)ì— ì—°ì†ì ì¸ í…ìŠ¤íŠ¸ê°€ ë§ì´ í¬í•¨ë˜ì–´ ìˆì–´ ëª¨ë¸ì´ ì¥ê±°ë¦¬ ì¢…ì†ì„±(long range dependencies)ì„ í•™ìŠµí•˜ëŠ”ë° ë„ì›€

**`BPE: Byte Pair Encoding`**

- ê¸°ì¡´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ embedding ë°©ë²•ë³´ë‹¤ ì§„í™”ëœ ë°©ë²• ì‚¬ìš©

  - **ê¸°ì¡´**: word embedding ë˜ëŠ” character embedding

    - word embedding: ì‹ ì¡°ì–´, ì˜¤íƒˆìì— ì•½í•¨

      â†’ `OOV ë¬¸ì œ` OOV(Out-Of-Vocabulary) ë˜ëŠ” UNK(Unknown Token)

    - character embedding: ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ê°€ word embeddingì— ë¹„í•´ ë–¨ì–´ì§

  - **GPT-1**: Byte Pair Encoding

    - Byte Pair Encoding

      ëŒ€í‘œì ì¸ ì„œë¸Œì›Œë“œ ë¶„ë¦¬(Subword segmentation) ì•Œê³ ë¦¬ì¦˜

      : í•˜ë‚˜ì˜ ë‹¨ì–´ëŠ” ë” ì‘ì€ ë‹¨ìœ„ì˜ ì˜ë¯¸ìˆëŠ” ì—¬ëŸ¬ ì„œë¸Œì›Œë“œë“¤(Ex) birthplace = birth + place)ì˜ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì—, í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì—¬ëŸ¬ ì„œë¸Œì›Œë“œë¡œ ë¶„ë¦¬í•´ì„œ ë‹¨ì–´ë¥¼ ì¸ì½”ë”© ë° ì„ë² ë”©í•˜ê² ë‹¤ëŠ” ì˜ë„ë¥¼ ê°€ì§„ ì „ì²˜ë¦¬ ì‘ì—…

      â†’ `OOV`ë‚˜ í¬ê·€ ë‹¨ì–´, ì‹ ì¡°ì–´ì™€ ê°™ì€ ë¬¸ì œ `ì™„í™”`

      â†’ ì…ë ¥ê°’ì˜ ì˜ë¯¸ ë” ì˜ ì „ë‹¬

### 2. Supervised Fine-tuning

Unsupervised pre-trainingì—ì„œ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¬ì‚¬ìš©í–ˆë‹¤.

**`Four types of language understanding tasks`**

- **Natural Language Inference (NLI)**

  - í…ìŠ¤íŠ¸ í•¨ì˜ ì¸ì‹(recognizing textual entailment)ìœ¼ë¡œë„ ì•Œë ¤ì ¸ìˆìœ¼ë©°, ë‘ ë¬¸ì¥ ê°„ì˜ ê´€ê³„ë¥¼ ë§ì¶”ëŠ” task

  - Labelì€ 3ê°€ì§€ ì¢…ë¥˜ ì¡´ì¬: Contradiction / Neutral / Entailment

    _datasets: SNLI, transcribed speech, popular fiction, MNLI, QNLI, SciTail, RTE_

  - Examples

    <img width="681" alt="img" src="https://user-images.githubusercontent.com/53266682/130374277-1feba1ca-e9d1-4a4f-b8a7-b083c76c161c.png">
    
    _ì¶œì²˜ https://github.com/kakaobrain/KorNLUDatasets_

- **Question answering and commonsense reasoning**

  - ì§€ë¬¸ê³¼ ì´ì— ê´€ë ¨ëœ ì§ˆë¬¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ì•Œë§ì€ ë‹µì„ ì°¾ì•„ë‚´ëŠ” task

    _dataset: RACE_

  - Examples

    <img src="https://rajpurkar.github.io/mlx/qa-and-squad/example-squad.png" alt="img" width="70%" />
    
    _ì¶œì²˜ https://rajpurkar.github.io/mlx/qa-and-squad/_

- **Semantic Similarity**

  - ì£¼ì–´ì§„ ë‘ ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬í•œ ì •ë„ë¥¼ ì ìˆ˜ë¡œ ê³„ì‚°

    _datasets: MRPC, QQP, STS-B_

  - Examples

    <img src="https://2.bp.blogspot.com/-9Qk1fubLpzg/Wv2QGgKVVmI/AAAAAAAACvs/Gm-XF3prXVIIvaIkrTmkcIcYz-4qSxLKwCLcBGAs/s1600/image2.png" alt="img" width="50%" />

    _ì¶œì²˜ https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html_

- **Classification**

  - CoLA dataset: ë¬¸ì¥ì´ ë¬¸ë²•ì ìœ¼ë¡œ ë§ì•˜ëŠ”ì§€ í‹€ë ¸ëŠ”ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” task
  - SST-2 dataset: í‘œì¤€ ì´ì§„ ë¶„ë¥˜ taskë¡œ, ë¬¸ì¥ì˜ sentimentë¥¼ ë¶„ë¥˜

  - Examples

    <img src="https://paperswithcode.com/media/datasets/sst.jpg" alt="img" width="50%" />

    _ì¶œì²˜ https://paperswithcode.com/dataset/sst_

**â‡’ 12ê°œ ë¶€ë¬¸ ì¤‘ 9ê°œ ë¶€ë¬¸ì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆë‹¤.**

## Analysis

**`Impact of number of layers transferred`**

unsupervised pre-trainingì—ì„œ supervised target taskë¡œ ë‹¤ì–‘í•œ ìˆ˜ì˜ ë ˆì´ì–´ë¥¼ ì „ì´(transfer)í–ˆì„ ë•Œì˜ ì˜í–¥ì„ ê´€ì°°í•œ ê²°ê³¼

- Layerì˜ ê°œìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì •í™•ë„ê°€ í–¥ìƒë˜ì—ˆë‹¤.
- Layer #12 ì´í›„ë¶€í„°ëŠ” ìˆ˜ë ´ ì–‘ìƒì„ ë³´ì˜€ë‹¤.  _(Cf. ëª¨ë¸ ì•„í‚¤í…ì²˜: Transformerì˜ decoderë¥¼ 12ê°œ ìŒ“ì€ êµ¬ì¡°)_

![GPT1-Figure2](https://user-images.githubusercontent.com/53266682/130374819-ef287ae8-495a-4240-910e-5ebcabd71468.png)

**`Zero-shot Behaviors of the pre-trained model`** 

Transformerë¡œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì€ downstream tasksì— ìœ ìš©í•œ ì–¸ì–´ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ì–»ëŠ”ë‹¤.

- LSTMì´ zero-shot ì„±ëŠ¥ì—ì„œ ë” í° varianceë¥¼ ë³´ì´ëŠ” ê²ƒì„ ê´€ì°°í•˜ì˜€ê³ , Transformer ì•„í‚¤í…ì²˜ê°€ LSTMë³´ë‹¤ ì „ì´(transfer)ë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ í•œë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•œë‹¤.

_Cf. Transfer learningê³¼ downstream task_

_ì „ì´ í•™ìŠµ(Transfer Learning)ì´ë€ íŠ¹ì • íƒœìŠ¤í¬ë¥¼ í•™ìŠµí•œ ëª¨ë¸ì„ ë‹¤ë¥¸ íƒœìŠ¤í¬ ìˆ˜í–‰ì— ì¬ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì„ ê°€ë¦¬í‚¨ë‹¤. ëª¨ë¸ì´ ìƒˆë¡œìš´ íƒœìŠ¤í¬(`Task2`)ë¥¼ ë°°ìš¸ ë•Œ, ì´ì „ì— íƒœìŠ¤í¬(`Task1`)ë¥¼ ìˆ˜í–‰í•´ë´¤ë˜ ê²½í—˜ì„ ì¬ì‚¬ìš©í•œë‹¤ê³  í–ˆì„ ë•Œ, `Task1`ì€ **upstream task**, `Task2`ëŠ” **downstream task**ë¼ê³  í•œë‹¤._

_Upstream taskë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì‚¬ì „ í•™ìŠµ(**pretrain**)ì´ë¼ê³  í•˜ê³ , downstream taskë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì€ ë°©ì‹ì— ë”°ë¼ ì—¬ëŸ¬ ê°€ì§€ ìš©ì–´ë¡œ ë¶ˆë¦°ë‹¤. **Fine tuning, zero-shot learning, one-shot learning, few-shot learning** ë“±ì´ ìˆë‹¤. ì´ ê¸€ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì¶”ê°€ì ì¸ ì„¤ëª…ì„ ë‹¬ì•„ë†“ì•˜ë‹¤._

_ìœ„ ì„¤ëª…ì€ [ratsgo's NLPBOOK](https://ratsgo.github.io/nlpbook/docs/introduction/transfer/)ì„ ì°¸ê³ í•˜ì˜€ìœ¼ë©°, Transfer learningì— ëŒ€í•œ ë” ìì„¸í•œ ì„¤ëª…ì´ ë³´ê³  ì‹¶ë‹¤ë©´ ì´ ì‚¬ì´íŠ¸ì—ì„œ ì‚´í´ë³´ì_.

**`Alblation studies`**

_Cf. Ablation studyë€? ì „ì²´ ì‹œìŠ¤í…œì— ëŒ€í•œ êµ¬ì„± ìš”ì†Œì˜ ê¸°ì—¬ë„ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ íŠ¹ì • êµ¬ì„± ìš”ì†Œë¥¼ ì œê±°í•˜ì—¬ AI ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì—°êµ¬í•˜ëŠ” ê²ƒì´ë‹¤._

![GPT1-Table5-1](https://user-images.githubusercontent.com/53266682/130374825-e98fa781-0523-42a5-8438-e06ea09ae366.png)

_Classification tasks: CoLA, SST2  | Semantic Similarity tasks: MRPC, STSB, QQP | NLI tasks: NMLI, QNLI, RTE_

1. Pre-training ì—†ì´ directly trained on supervised target tasks ê²½ìš° ì„±ëŠ¥ ë¹„êµ
   - ëª¨ë“  taskì— ëŒ€í•´ pre-trainingì´ ì—†ìœ¼ë©´ ì„±ëŠ¥ì´ ì €í•˜ëœë‹¤. (14.8%)
2. Fine-tuning ì‹œì— ë³´ì¡° ëª©ì  í•¨ìˆ˜(auxiliary LM objective)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì„±ëŠ¥ ê²€ì‚¬
   - auxiliary objectiveì„ ì´ìš©í•˜ëŠ” ê²ƒì´ í° ë°ì´í„°ì…‹ì—ëŠ” íš¨ê³¼ì ì´ì§€ë§Œ, ì‘ì€ ë°ì´í„°ì…‹ì—ëŠ” ì•„ë‹ˆë‹¤.
3. Single layer 2048 unit LSTMê³¼ ë¹„êµí•˜ì—¬ Transformerì˜ íš¨ê³¼ ë¶„ì„
   - Transformer ëŒ€ì‹  LSTMì„ ì´ìš©í•˜ë©´ average scoreê°€ 5.6ë§Œí¼ ë–¨ì–´ì§„ë‹¤.

â‡’ ì‚¬ì „ í•™ìŠµì´ ì„±ëŠ¥ í–¥ìƒì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹˜ê³ , LSTMë³´ë‹¤ Transformerë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ì´ ì¢‹ë‹¤. Auxiliary object ì‚¬ìš©ì€ í° ë°ì´í„°ì…‹ì—ëŠ” íš¨ê³¼ì ì§€ë§Œ ì‘ì€ ë°ì´í„°ì…‹ì—ëŠ” ì•„ë‹ˆë‹¤.

## Conclusion

**`GPT-1`**

- Transformerì˜ decoderë¥¼ ì‚¬ìš©
- Unsupervised Learning with unlabeled text for pre-training
- Fine tuning without additional task specific model

GPT-1ì€ ìƒì„±ì  ì‚¬ì „ í•™ìŠµ(generative pre-training)ì˜ í˜ì„ ë³´ì—¬ì£¼ì—ˆê³ , ë” í° ë°ì´í„°ì…‹ê³¼ ë” ë§ì€ ë§¤ê°œë³€ìˆ˜ë¡œ ì´ëŸ¬í•œ ì ì¬ë ¥ì„ ë” ì˜ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ëª¨ë¸ì— ëŒ€í•œ ê¸¸ì„ ì—´ì–´ì£¼ì—ˆë‹¤. ê·¸ë¦¬ê³  ì´ë“¬í•´ ë‚˜ì˜¨ GPT-2ê°€ ë°”ë¡œ ê·¸ëŸ¬í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë‹¤.

---

## ì¶”ê°€

### Fine-tuning, N-shot learning

ì „ì´ í•™ìŠµ(Transfer Learning)ì—ì„œ downstream taskë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹

- **Fine-tuning**  downstream taskì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° **ì „ì²´**ë¥¼ ì‚¬ìš©í•œë‹¤. ëª¨ë¸ ì „ì²´ë¥¼ downstream ë°ì´í„°ì— ë§ê²Œ ì—…ë°ì´íŠ¸í•œë‹¤.
- **Zero-shot learning**  downstream task ë°ì´í„°ë¥¼ **ì „í˜€** ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ëª¨ë¸ì´ ë°”ë¡œ downstream taskë¥¼ ìˆ˜í–‰í•œë‹¤.
- **One-shot learning**  downstream task ë°ì´í„°ë¥¼ **í•œ ê±´ë§Œ** ì‚¬ìš©í•œë‹¤. ëª¨ë¸ ì „ì²´ë¥¼ í•œ ê±´ì˜ ë°ì´í„°ì— ë§ê²Œ ì—…ë°ì´íŠ¸í•œë‹¤. ì—…ë°ì´íŠ¸ ì—†ì´ ìˆ˜í–‰í•˜ëŠ” one-shot learningë„ ìˆë‹¤. ëª¨ë¸ì´ 1ê±´ì˜ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìˆ˜í–‰ë˜ëŠ”ì§€ ì°¸ê³ í•œ ë’¤ ë°”ë¡œ downstream taskë¥¼ ìˆ˜í–‰í•œë‹¤.
- **Few-shot learning**  downstream task ë°ì´í„°ë¥¼ **ëª‡ ê±´ë§Œ** ì‚¬ìš©í•œë‹¤. ëª¨ë¸ ì „ì²´ë¥¼ ëª‡ ê±´ì˜ ë°ì´í„°ì— ë§ê²Œ ì—…ë°ì´íŠ¸í•œë‹¤. ì—…ë°ì´íŠ¸ ì—†ì´ ìˆ˜í–‰í•˜ëŠ” few-shot learningë„ ìˆë‹¤. ëª¨ë¸ì´ ëª‡ ê±´ì˜ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìˆ˜í–‰ë˜ëŠ”ì§€ ì°¸ê³ í•œ ë’¤ ë°”ë¡œ downstream taskë¥¼ ìˆ˜í–‰í•œë‹¤.

_ì¶œì²˜ [ratsgo's NLPBOOK](https://ratsgo.github.io/nlpbook/docs/introduction/transfer/)_

### OpenAI GPT models

1. **GPT-1** paper ([Improving Language Understanding by Generative Pre-training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)).
   - Fine-tuning
   - ë§¤ê°œë³€ìˆ˜: ì•½ 1ì–µ 1700ë§Œ ê°œ
   - í™œìš©
     - ì£¼ì–´ì§„ ë‘ ë¬¸ì¥ì˜ ê´€ê³„ ìœ ì¶”
     - ì£¼ì–´ì§„ ë‘ ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
     - í•˜ë‚˜ì˜ ì •ë³´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë‹µ ì°¾ê¸°
     - íŠ¹ì • ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜

2. **GPT-2** paper ([Language Models are unsupervised multitask learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)) 

   GPT-1ì— ëŒ€í•œ í›„ì† ê°œì„ ì‚¬í•­

   - Zero-shot learning
   - ë§¤ê°œë³€ìˆ˜: ì•½ 15ì–µ ê°œ
   - í™œìš©
     - ë…í•´ (Reading Comprehension)
     - ê¸€ ì§“ê¸°

3. **GPT-3** paper [(Language models are few shot learners](https://arxiv.org/pdf/2005.14165.pdf)) 

   ì˜¤ëŠ˜ë‚ ê¹Œì§€ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ê°€ì¥ ê°•ë ¥í•œ ëª¨ë¸ ì¤‘ í•˜ë‚˜

   - Few-shot learning
     - fine-tuning ì—†ì´ë„ ì—„ì²­ë‚œ ì„±ëŠ¥ ë°œíœ˜
   - ë§¤ê°œë³€ìˆ˜: ì•½ 1750ì–µ ê°œ
   - í™œìš©
     - ê¸°ì‚¬ ì‘ì„±
     - ìƒì‹ Q&A
     - ê²€ìƒ‰ ì—”ì§„
     - ëŒ€í™”
     - í…ìŠ¤íŠ¸ ìš”ì•½

_ìœ„ ë…¼ë¬¸ë“¤ì€ [ì—¬ê¸°](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2)ì— ìˆœì°¨ì ìœ¼ë¡œ ì˜ ì •ë¦¬ë˜ì–´ ìˆë‹¤._

_Cf. GPT-1, GPT-2ì™€ ë¹„êµí•œ GPT-3ì˜ ì–´ë§ˆì–´ë§ˆí•œ íŒŒë¼ë¯¸í„° ê°œìˆ˜_

<img src="https://research.aimultiple.com/wp-content/uploads/2021/01/number-of-model-parameters-from-Elmo-to-Turing-NLG-1536x917.png" alt="img" style="zoom:40%;" /><img src="https://research.aimultiple.com/wp-content/uploads/2021/01/number-of-model-parameters-until-gpt-3.png" alt="img" />

_ì¶œì²˜ https://research.aimultiple.com/gpt/_
  

## 	ì°¸ê³  ìë£Œ

**ë…¼ë¬¸**  [Improving Language Understanding by Generative Pre-Training](https://arxiv.org/pdf/1706.03762.pdf)

**ì˜ìƒ**  [GPT-1 (ë°‘ë°”ë‹¥ë¶€í„° ì•Œì•„ë³´ëŠ” GPT 1ê°•) Minsuk Heo í—ˆë¯¼ì„](https://www.youtube.com/watch?v=FeEmmylAF0o_) ğŸŒŸ

**ì˜ìƒ**  [[Paper Review] Improving Language Understanding by Generative Pre-Training](https://www.youtube.com/watch?v=4qv_ofZN5_U) ğŸŒŸ

**ë¬¸ì„œ**  [The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2) ğŸŒŸ

**ë¬¸ì„œ**  [ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ / ë°”ì´íŠ¸ í˜ì–´ ì¸ì½”ë”©](https://wikidocs.net/22592)

**ë¬¸ì„œ**  [ratsgo's NLPBOOK](https://ratsgo.github.io/nlpbook/docs/introduction/transfer/) ğŸŒŸ

**ë¬¸ì„œ**  [ratsgo's blog / discriminative vs generative](https://ratsgo.github.io/generative%20model/2017/12/17/compare/)

